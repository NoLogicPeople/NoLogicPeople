import json
import os
from typing import Tuple, List, Optional

class Guard:
    """
    LLM girdilerini PII, prompt injection ve diÄŸer istenmeyen kalÄ±plara
    karÅŸÄ± korumak iÃ§in yapÄ±landÄ±rÄ±labilir bir gÃ¼venlik katmanÄ±.
    """
    def __init__(self, config_path: str = "config.json") -> None:
        """
        Guard'Ä± belirtilen yapÄ±landÄ±rma dosyasÄ±ndan baÅŸlatÄ±r.
        """
        # Config dosyasÄ± Ã¶ncelik sÄ±rasÄ±:
        # 1) GUARD_CONFIG_PATH ortam deÄŸiÅŸkeni
        # 2) app/security/config.json (bu dosya ile aynÄ± klasÃ¶rde)
        base_dir = os.path.dirname(__file__)
        default_path = config_path
        if not os.path.isabs(default_path):
            default_path = os.path.join(base_dir, default_path)

        env_path = os.getenv("GUARD_CONFIG_PATH")
        if env_path and not os.path.isabs(env_path):
            env_path = os.path.join(base_dir, env_path)

        candidate_paths = [p for p in [env_path, default_path] if p]

        config = {}
        loaded = False
        for candidate in candidate_paths:
            try:
                with open(candidate, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    loaded = True
                    break
            except FileNotFoundError:
                continue
            except json.JSONDecodeError as e:
                print(f"âš ï¸ Guard yapÄ±landÄ±rma dosyasÄ± geÃ§erli bir JSON deÄŸil ('{candidate}'): {e}. VarsayÄ±lan deÄŸerler kullanÄ±lacak.")
                break

        if not loaded:
            # Dosya bulunamazsa gÃ¼venli varsayÄ±lanlarla devam et
            print(f"âš ï¸ Guard yapÄ±landÄ±rma dosyasÄ± bulunamadÄ±. Åu yollar denendi: {candidate_paths}. VarsayÄ±lan deÄŸerler kullanÄ±lacak.")

        # Temel yapÄ±landÄ±rmadan deÄŸerleri al
        self._max_length: int = config.get("max_prompt_length", 5000)
        self._pii_keywords: List[str] = config.get("pii_keywords", [])
        self._prompt_injection_keywords: List[str] = config.get("prompt_injection_keywords", [])
        
        # NLP gÃ¼venlik ayarlarÄ±
        nlp_config = config.get("nlp_security", {})
        self._enable_ner = nlp_config.get("enable_ner", False)
        self._enable_semantic = nlp_config.get("enable_semantic_similarity", False)
        self._enable_toxicity = nlp_config.get("enable_toxicity_detection", False)
        self._ner_threshold = nlp_config.get("ner_confidence_threshold", 0.85)
        self._similarity_threshold = nlp_config.get("similarity_threshold", 0.75)
        self._toxicity_threshold = nlp_config.get("toxicity_threshold", 0.8)
        self._attack_prompts = nlp_config.get("known_attack_prompts", [])
        
        # NLP modelleri (lazy loading)
        self.ner_pipeline = None
        self.similarity_model = None
        self.attack_embeddings = None
        self.toxicity_pipeline = None
        
        # NLP modellerini yÃ¼kle (eÄŸer aktifse)
        self._init_nlp_models()

    def _init_nlp_models(self) -> None:
        """NLP modellerini lazy loading ile baÅŸlatÄ±r."""
        if self._enable_ner:
            try:
                print("ğŸ”’ TÃ¼rkÃ§e NER modeli yÃ¼kleniyor...")
                from transformers import pipeline
                # TÃ¼rkÃ§e iÃ§in fine-tune edilmiÅŸ BERT NER modeli
                self.ner_pipeline = pipeline("ner", model="savasy/bert-base-turkish-ner-cased", 
                                           aggregation_strategy="simple", device=-1)
                print("âœ… TÃ¼rkÃ§e NER modeli hazÄ±r")
            except Exception as e:
                print(f"âš ï¸ TÃ¼rkÃ§e NER modeli yÃ¼klenemedi: {e}")
                # Fallback: Multilingual model
                try:
                    print("ğŸ”„ Ã‡ok dilli NER modeline geÃ§iliyor...")
                    self.ner_pipeline = pipeline("ner", model="Davlan/xlm-roberta-base-ner-hrl", 
                                               aggregation_strategy="simple", device=-1)
                    print("âœ… Ã‡ok dilli NER modeli hazÄ±r")
                except Exception as e2:
                    print(f"âš ï¸ NER modeli tamamen yÃ¼klenemedi: {e2}")
                    self._enable_ner = False
        
        if self._enable_semantic and self._attack_prompts:
            try:
                print("ğŸ”’ Semantik benzerlik modeli yÃ¼kleniyor...")
                from sentence_transformers import SentenceTransformer
                import torch
                
                self.similarity_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                
                # Attack prompts'larÄ± gÃ¼venli ÅŸekilde encode et
                embeddings = self.similarity_model.encode(self._attack_prompts, convert_to_tensor=True)
                
                # Tensor boyutlarÄ±nÄ± kontrol et
                if isinstance(embeddings, torch.Tensor):
                    if len(embeddings.shape) == 1:
                        embeddings = embeddings.unsqueeze(0)
                    self.attack_embeddings = embeddings
                else:
                    # NumPy array ise tensor'a Ã§evir
                    import numpy as np
                    if isinstance(embeddings, np.ndarray):
                        self.attack_embeddings = torch.tensor(embeddings)
                    else:
                        self.attack_embeddings = embeddings
                
                print("âœ… Semantik model hazÄ±r")
            except Exception as e:
                print(f"âš ï¸ Semantik model yÃ¼klenemedi: {e}")
                self._enable_semantic = False
        
        if self._enable_toxicity:
            try:
                print("ğŸ”’ TÃ¼rkÃ§e toksisite modeli yÃ¼kleniyor...")
                from transformers import pipeline
                # TÃ¼rkÃ§e iÃ§in Ã¶zel eÄŸitilmiÅŸ toksisite modeli
                self.toxicity_pipeline = pipeline("text-classification", 
                                                 model="savasy/bert-base-turkish-sentiment-cased", device=-1)
                print("âœ… TÃ¼rkÃ§e toksisite modeli hazÄ±r")
            except Exception as e:
                print(f"âš ï¸ TÃ¼rkÃ§e toksisite modeli yÃ¼klenemedi: {e}")
                # Fallback: Ã‡ok dilli toksisite modeli
                try:
                    print("ğŸ”„ Ã‡ok dilli toksisite modeline geÃ§iliyor...")
                    self.toxicity_pipeline = pipeline("text-classification", 
                                                     model="unitary/multilingual-toxic-xlm-roberta", device=-1)
                    print("âœ… Ã‡ok dilli toksisite modeli hazÄ±r")
                except Exception as e2:
                    print(f"âš ï¸ Toksisite modeli tamamen yÃ¼klenemedi: {e2}")
                    self._enable_toxicity = False

    def _check_length(self, text: str) -> Optional[str]:
        """Girdi metninin uzunluÄŸunu kontrol eder."""
        if not text:
            return "BoÅŸ istek."
        if len(text) > self._max_length:
            return f"Ä°stek Ã§ok uzun (Maks: {self._max_length} karakter)."
        return None

    def _check_keywords(self, text: str) -> Optional[str]:
        """YapÄ±landÄ±rÄ±lmÄ±ÅŸ anahtar kelime listelerine gÃ¶re metni kontrol eder."""
        # PII anahtar kelime kontrolÃ¼ kaldÄ±rÄ±ldÄ± - sadece NER kullanÄ±lacak
        if any(keyword in text for keyword in self._prompt_injection_keywords):
            return "Kural atlama giriÅŸimi algÄ±landÄ±."
        return None

    def _check_ner_entities(self, text: str) -> Optional[str]:
        """NER ile hassas varlÄ±klarÄ± tespit eder."""
        if not self._enable_ner or not self.ner_pipeline:
            return None
            
        try:
            entities = self.ner_pipeline(text)
            # Sadece kiÅŸi adlarÄ±nÄ± kontrol et - TCKN, yer ve kurum adlarÄ± e-Devlet iÃ§in normaldir
            sensitive_entities = {'PER', 'PERSON'}
            
            for entity in entities:
                entity_type = entity.get('entity_group', '').upper()
                confidence = entity.get('score', 0)
                entity_word = entity.get('word', '').strip()
                
                # KiÅŸi adÄ± tespiti ve TCKN benzeri sayÄ±larÄ± filtrele
                if entity_type in sensitive_entities and confidence > self._ner_threshold:
                    # TCKN benzeri 11 haneli sayÄ±larÄ± ignore et
                    if entity_word.isdigit() and len(entity_word) == 11:
                        continue
                    # KÄ±sa sayÄ±sal deÄŸerleri ignore et (kimlik no, dosya no vs.)
                    if entity_word.isdigit() and len(entity_word) <= 15:
                        continue
                    # GerÃ§ek kiÅŸi adÄ± tespit edildi
                    return f"KiÅŸi adÄ± algÄ±landÄ± (Ad: {entity_word})"
        except Exception as e:
            print(f"NER kontrolÃ¼ baÅŸarÄ±sÄ±z: {e}")
        
        return None

    def _check_semantic_similarity(self, text: str) -> Optional[str]:
        """Semantik benzerlik ile prompt injection tespiti."""
        if not self._enable_semantic or not self.similarity_model or self.attack_embeddings is None:
            return None
            
        try:
            from sentence_transformers import util
            import torch
            
            # Text'i encode et
            text_embedding = self.similarity_model.encode(text, convert_to_tensor=True)
            
            # Tensor boyutlarÄ±nÄ± kontrol et ve dÃ¼zelt
            if len(text_embedding.shape) == 1:
                text_embedding = text_embedding.unsqueeze(0)
            
            # Cosine similarity hesapla
            cosine_scores = util.cos_sim(text_embedding, self.attack_embeddings)
            
            # Max score'u gÃ¼venli ÅŸekilde al
            if isinstance(cosine_scores, torch.Tensor):
                max_score = torch.max(cosine_scores).item()
            else:
                max_score = float(cosine_scores.max())
            
            if max_score > self._similarity_threshold:
                return f"Kural atlama giriÅŸimi algÄ±landÄ± (Benzerlik Skoru: {max_score:.2f})"
                
        except Exception as e:
            print(f"Semantik benzerlik kontrolÃ¼ baÅŸarÄ±sÄ±z: {e}")
        
        return None

    def _check_toxicity(self, text: str) -> Optional[str]:
        """Toksisite ve zararlÄ± iÃ§erik tespiti."""
        if not self._enable_toxicity or not self.toxicity_pipeline:
            return None
            
        try:
            results = self.toxicity_pipeline(text)
            
            # Model Ã§Ä±ktÄ±sÄ±na gÃ¶re kontrol
            for result in results:
                label = result.get('label', '').lower()
                score = result.get('score', 0)
                
                # TÃ¼rkÃ§e sentiment modeli: negative sentiment yÃ¼ksekse toksik olabilir
                if 'negative' in label and score > self._toxicity_threshold:
                    return f"Olumsuz/toksik iÃ§erik algÄ±landÄ± (Skor: {score:.2f})"
                elif 'toxic' in label and score > self._toxicity_threshold:
                    return f"Toksik iÃ§erik algÄ±landÄ± (Skor: {score:.2f})"
                elif 'hate' in label and score > self._toxicity_threshold:
                    return f"Nefret sÃ¶ylemi algÄ±landÄ± (Skor: {score:.2f})"
        except Exception as e:
            print(f"Toksisite kontrolÃ¼ baÅŸarÄ±sÄ±z: {e}")
        
        return None


    def check_prompt_safety(self, text: str) -> Tuple[bool, str]:
        """
        Girdi metnini tÃ¼m gÃ¼venlik kontrollerinden geÃ§irir.
        Herhangi bir kontrol baÅŸarÄ±sÄ±z olursa, iÅŸlemi durdurur ve nedenini dÃ¶ndÃ¼rÃ¼r.

        Returns:
            (is_safe, reason) tuple'Ä±.
        """
        if not text:
            return False, "BoÅŸ istek."
            
        # Gelen metni her seferinde temizle ve kÃ¼Ã§Ã¼k harfe Ã§evir
        processed_text = text.lower().strip()
        
        # Katman 1: HÄ±zlÄ± kontroller (uzunluk, temel anahtar kelimeler)
        basic_checks = [
            lambda t: self._check_length(t),
            lambda t: self._check_keywords(t),
        ]

        for check_func in basic_checks:
            reason = check_func(processed_text)
            if reason:
                return False, reason

        # Katman 2-4: NLP tabanlÄ± akÄ±llÄ± kontroller (orijinal metin ile)
        nlp_checks = [
            lambda t: self._check_ner_entities(t),
            lambda t: self._check_semantic_similarity(t),
            lambda t: self._check_toxicity(t),
        ]

        for check_func in nlp_checks:
            reason = check_func(text)  # Orijinal metin (bÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harf duyarlÄ±)
            if reason:
                return False, reason

        return True, "OK"